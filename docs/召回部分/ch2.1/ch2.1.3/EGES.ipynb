{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_session(data, time_cut=30, cut_type=2):\n",
    "    # 商品属性  id  被交互时间   商品种类\n",
    "    sku_list = data['sku_id']\n",
    "    time_list = data['action_time']\n",
    "    type_list = data['type']\n",
    "    session = []\n",
    "    tmp_session = []\n",
    "    for i, item in enumerate(sku_list):\n",
    "        # 两个商品之间如果被交互的时间大于1小时，划分成不同的session\n",
    "        if type_list[i] == cut_type or (i < len(sku_list)-1 and \\\n",
    "            (time_list[i+1] - time_list[i]).seconds/60 > time_cut) or i == len(sku_list)-1:\n",
    "            tmp_session.append(item)\n",
    "            session.append(tmp_session)\n",
    "            tmp_session = []\n",
    "        else:\n",
    "            tmp_session.append(item)\n",
    "    return session  # 返回多个session list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pair = dict()\n",
    "# 遍历所有session list\n",
    "for session in session_list_all:\n",
    "    for i in range(1, len(session)):\n",
    "        # 将session共现的item存到node_pair中，用于构建item-item图\n",
    "        # 将共现次数所谓边的权重，即node_pair的key为边(src_node,dst_node),value为边的权重(共现次数)\n",
    "        if (session[i - 1], session[i]) not in node_pair.keys():\n",
    "            node_pair[(session[i - 1], session[i])] = 1\n",
    "        else:\n",
    "            node_pair[(session[i - 1], session[i])] += 1\n",
    "\n",
    "in_node_list = list(map(lambda x: x[0], list(node_pair.keys())))#计算入度\n",
    "out_node_list = list(map(lambda x: x[1], list(node_pair.keys())))#计算出度\n",
    "weight_list = list(node_pair.values())#权重\n",
    "graph_list = list([(i,o,w) for i,o,w in zip(in_node_list,out_node_list,weight_list)])\n",
    "# 通过 network 构建图结构\n",
    "G = nx.DiGraph().add_weighted_edges_from(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "node_pair = {(2,3):10}\n",
    "in_node_list = list(map(lambda x: x[0], list(node_pair.keys())))\n",
    "print(in_node_list)  # 输出: [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walker = RandomWalker(G, p=args.p, q=args.q)\n",
    "print(\"Preprocess transition probs...\")\n",
    "walker.preprocess_transition_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transition_probs(self):\n",
    "    \"\"\"预处理随即游走的转移概率\"\"\"\n",
    "    G = self.G\n",
    "    alias_nodes = {}\n",
    "    for node in G.nodes():\n",
    "        # 获取每个节点与邻居节点边上的权重\n",
    "        unnormalized_probs = [G[node][nbr].get('weight', 1.0)   \n",
    "                                for nbr in G.neighbors(node)]\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "         # 对每个节点的邻居权重进行归一化\n",
    "        normalized_probs = [\n",
    "            float(u_prob)/norm_const for u_prob in unnormalized_probs] \n",
    "        # 根据权重创建alias表\n",
    "        alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "    alias_edges = {}\n",
    "    for edge in G.edges():\n",
    "        # 获取边的alias\n",
    "        alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "    self.alias_nodes = alias_nodes\n",
    "    self.alias_edges = alias_edges\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_reproduce = walker.simulate_walks(num_walks=args.num_walks, \n",
    "            walk_length=args.walk_length, workers=4,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simulate_walks(self, nodes, num_walks, walk_length,):\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        # 打乱所有起始节点\n",
    "        random.shuffle(nodes)\n",
    "        for v in nodes:\n",
    "            # 根据p和q选择随机游走或者带权游走\n",
    "            if self.p == 1 and self.q == 1:\n",
    "                walks.append(self.deepwalk_walk(\n",
    "                    walk_length=walk_length, start_node=v))\n",
    "            else:\n",
    "                walks.append(self.node2vec_walk(\n",
    "                    walk_length=walk_length, start_node=v))\n",
    "    return walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_side_info = pd.merge(all_skus, product_data, on='sku_id', how='left').fillna(0) # 为商品加载side information\n",
    "for feat in sku_side_info.columns:\n",
    "    if feat != 'sku_id':\n",
    "        lbe = LabelEncoder()\n",
    "        # 对side information进行编码\n",
    "        sku_side_info[feat] = lbe.fit_transform(sku_side_info[feat])\n",
    "    else:\n",
    "        sku_side_info[feat] = sku_lbe.transform(sku_side_info[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_context_all_pairs(walks, window_size):\n",
    "    all_pairs = []\n",
    "    for k in range(len(walks)):\n",
    "        for i in range(len(walks[k])):\n",
    "            # 通过窗口的方式采取正样本，具体的是，让随机游走序列的起始item与窗口内的每个item组成正样本对\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                if i == j or j < 0 or j >= len(walks[k]):\n",
    "                    continue\n",
    "                else:\n",
    "                    all_pairs.append([walks[k][i], walks[k][j]])\n",
    "    return np.array(all_pairs, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGES(side_information_columns, items_columns, merge_type = \"weight\", share_flag=True,\n",
    "        l2_reg=0.0001, seed=1024):\n",
    "    # side_information 所对应的特征\n",
    "    feature_columns = list(set(side_information_columns))\n",
    "    # 获取输入层，查字典\n",
    "    feature_encode = FeatureEncoder(feature_columns,  linear_sparse_feature=None)\n",
    "    # 输入的值\n",
    "    feature_inputs_list = list(feature_encode.feature_input_layer_dict.values())\n",
    "    # item id  获取输入层的值\n",
    "    items_Map = FeatureMap(items_columns)\n",
    "    items_inputs_list = list(items_Map.feature_input_layer_dict.values())\n",
    "\n",
    "    # 正样本的id，在softmax中需要传入正样本的id\n",
    "    label_columns = [DenseFeat('label_id', 1)]\n",
    "    label_Map = FeatureMap(label_columns)\n",
    "    label_inputs_list = list(label_Map.feature_input_layer_dict.values())\n",
    "\n",
    "    # 通过输入的值查side_information的embedding，返回所有side_information的embedding的list\n",
    "    side_embedding_list = process_feature(side_information_columns, feature_encode)\n",
    "    # 拼接  N x num_feature X Dim\n",
    "    side_embeddings = Concatenate(axis=1)(side_embedding_list)\n",
    "\n",
    "    # items_inputs_list[0] 为了查找每个item 用于计算权重的 aplha 向量\n",
    "    eges_inputs = [side_embeddings, items_inputs_list[0]]\n",
    "\n",
    "    merge_emb = EGESLayer(items_columns[0].vocabulary_size, merge_type=merge_type, \n",
    "                l2_reg=l2_reg, seed=seed)(eges_inputs)  # B * emb_dim\n",
    "    \n",
    "    label_idx = label_Map.feature_input_layer_dict[label_columns[0].name]\n",
    "    softmaxloss_inputs = [merge_emb,label_idx]\n",
    "    \n",
    "    item_vocabulary_size = items_columns[0].vocabulary_size\n",
    "\n",
    "    all_items_idx = EmbeddingIndex(list(range(item_vocabulary_size)))\n",
    "    all_items_embeddings = feature_encode.embedding_layers_dict[side_information_columns[0].name](all_items_idx)\n",
    "\n",
    "    if share_flag:\n",
    "        softmaxloss_inputs.append(all_items_embeddings)\n",
    "    \n",
    "    output = SampledSoftmaxLayer(num_items=item_vocabulary_size, share_flage=share_flag,\n",
    "              emb_dim=side_information_columns[0].embedding_dim,num_sampled=10)(softmaxloss_inputs)\n",
    "\n",
    "    model = Model(feature_inputs_list + items_inputs_list + label_inputs_list, output)\n",
    "    \n",
    "    model.__setattr__(\"feature_inputs_list\", feature_inputs_list)\n",
    "    model.__setattr__(\"label_inputs_list\", label_inputs_list)\n",
    "    model.__setattr__(\"merge_embedding\", merge_emb)\n",
    "    model.__setattr__(\"item_embedding\", get_item_embedding(all_items_embeddings,                          \t\t\t\t\t\t\t\titems_Map.feature_input_layer_dict[items_columns[0].name]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGESLayer(Layer):\n",
    "    def __init__(self,item_nums, merge_type=\"weight\",l2_reg=0.001,seed=1024, **kwargs):\n",
    "        super(EGESLayer, self).__init__(**kwargs)\n",
    "        self.item_nums = item_nums \n",
    "        self.merge_type = merge_type   #聚合方式\n",
    "        self.l2_reg = l2_reg\n",
    "        self.seed = seed\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) < 2:\n",
    "            raise ValueError('`EGESLayer` layer should be called \\\n",
    "                on a list of at least 2 inputs')\n",
    "        self.feat_nums = input_shape[0][1]\n",
    "        \n",
    "        if self.merge_type == \"weight\":\n",
    "            self.alpha_embeddings = self.add_weight(\n",
    "                                name='alpha_attention',\n",
    "                                shape=(self.item_nums, self.feat_nums),\n",
    "                                dtype=tf.float32, \n",
    "                                initializer=tf.keras.initializers.RandomUniform(minval=-1, maxval=1,                                               seed=self.seed),\n",
    "                                regularizer=l2(self.l2_reg))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.merge_type == \"weight\": \n",
    "            stack_embedding = inputs[0]  # (B * num_feate * embedding_size)\n",
    "            item_input = inputs[1]       # (B * 1)  \n",
    "            alpha_embedding = tf.nn.embedding_lookup(self.alpha_embeddings, item_input) #(B * 1 * num_feate)\n",
    "            alpha_emb = tf.exp(alpha_embedding) \n",
    "            alpha_i_sum = tf.reduce_sum(alpha_emb, axis=-1) \n",
    "            merge_embedding = tf.squeeze(tf.matmul(alpha_emb, stack_embedding),axis=1) / alpha_i_sum\n",
    "        else:\n",
    "            stack_embedding = inputs[0]  # (B * num_feate * embedding_size)\n",
    "            merge_embedding = tf.squeeze(tf.reduce_mean(alpha_emb, axis=1),axis=1) # (B * embedding_size)\n",
    "        \n",
    "        return merge_embedding\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"merge_type\": self.merge_type, \"seed\": self.seed}\n",
    "        base_config = super(EGESLayer, self).get_config()\n",
    "        base_config.update(config)\n",
    "        return base_config\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
